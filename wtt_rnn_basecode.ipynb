{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wtt-rnn_basecode.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oFJH6WY-2zr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "37f0bec0-292d-49fb-ed28-ab9740771c44"
      },
      "source": [
        "!git clone https://github.com/dhyougit/keras-wtte-rnn.git"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-wtte-rnn'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Total 32 (delta 0), reused 0 (delta 0), pack-reused 32\u001b[K\n",
            "Unpacking objects: 100% (32/32), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QL4XnaQ9AYVF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "48c07c26-7833-48c5-b4aa-61eb38775e8f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_readme.txt  license.txt  test_x.csv  train.csv\n",
            "keras-wtte-rnn\t readme.md    test_y.csv  wtte-rnn.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvPWfB8s_nDi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "395e718f-449f-4812-8cb5-cf63ea08d53f"
      },
      "source": [
        "import os\n",
        "os.listdir()\n",
        "x = os.listdir(\"./keras-wtte-rnn\")\n",
        "x"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv',\n",
              " 'data_readme.txt',\n",
              " 'test_y.csv',\n",
              " 'readme.md',\n",
              " 'license.txt',\n",
              " '.git',\n",
              " 'test_x.csv',\n",
              " '.gitignore',\n",
              " 'wtte-rnn.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjUb3klM6FWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"./keras-wtte-rnn\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1aeko0T6JQJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "8628bf13-e2f2-44ab-f115-908df15bdea3"
      },
      "source": [
        "os.listdir()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['train.csv',\n",
              " 'data_readme.txt',\n",
              " 'test_y.csv',\n",
              " 'readme.md',\n",
              " 'license.txt',\n",
              " '.git',\n",
              " 'test_x.csv',\n",
              " '.gitignore',\n",
              " 'wtte-rnn.py']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmJvVwY2BycW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Masking\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as k\n",
        "from sklearn.preprocessing import normalize\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Syo53jmB8t3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "    Discrete log-likelihood for Weibull hazard function on censored survival data\n",
        "    y_true is a (samples, 2) tensor containing time-to-event (y), and an event indicator (u)\n",
        "    ab_pred is a (samples, 2) tensor containing predicted Weibull alpha (a) and beta (b) parameters\n",
        "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
        "\n",
        "    단절된 생존데이터에 대한 weibull위험 함수의 이산적 로그우도비.\n",
        "    y_true는 time-to-event값인 텐서 와 이벤트 지표이고 \n",
        "    ab_pred는 weibull분포의 알파값과 베타값을 예측한 텐서이다. \n",
        "    자세한 수학적 기반은 위의 깃에서 확인. \n",
        "\"\"\"\n",
        "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
        "    y_ = y_true[:, 0]\n",
        "    u_ = y_true[:, 1]\n",
        "    a_ = ab_pred[:, 0]\n",
        "    b_ = ab_pred[:, 1]\n",
        "\n",
        "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
        "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
        "\n",
        "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdmFZBeIynZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" 연속인경우 아래함수로.... \n",
        "    Not used for this model, but included in case somebody needs it\n",
        "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n",
        "\"\"\"\n",
        "def weibull_loglik_continuous(y_true, ab_pred, name=None):\n",
        "    y_ = y_true[:, 0]\n",
        "    u_ = y_true[:, 1]\n",
        "    a_ = ab_pred[:, 0]\n",
        "    b_ = ab_pred[:, 1]\n",
        "\n",
        "    ya = (y_ + 1e-35) / a_\n",
        "    return -1 * k.mean(u_ * (k.log(b_) + b_ * k.log(ya)) - k.pow(ya, b_))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2srVFhUyrTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "   이부분이 묘미. 케라스를 사용해서 아웃풋의 종류를 레이어 별로 다르게 설정해 주는것.. \n",
        "    Custom Keras activation function, outputs alpha neuron using exponentiation and beta using softplus\n",
        "\"\"\"\n",
        "def activate(ab):\n",
        "    a = k.exp(ab[:, 0])\n",
        "    b = k.softplus(ab[:, 1])\n",
        "\n",
        "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
        "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
        "\n",
        "    return k.concatenate((a, b), axis=1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoAY5rXgyv9n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f8644c8-14e0-4430-f47e-59ec76471529"
      },
      "source": [
        "\"\"\"\n",
        "데이터 전처리 과정. \n",
        "x끼리 합쳐서 정규화 한다음 다시 나누기.\n",
        "y에는 100일의 관찰 기관을 채우지 못한 데이터에 한해 0을 채움. 엄밀히 말하면 이부분 때문에  bias가 생길수는 있음.  \n",
        "    Load and parse engine data files into:\n",
        "       - an (engine/day, observed history, sensor readings) x tensor, where observed history is 100 days, zero-padded\n",
        "         for days that don't have a full 100 days of observed history (e.g., first observed day for an engine)\n",
        "       - an (engine/day, 2) tensor containing time-to-event and 1 (since all engines failed)\n",
        "    There are probably MUCH better ways of doing this, but I don't use Numpy that much, and the data parsing isn't the\n",
        "    point of this demo anyway.\n",
        "\"\"\"\n",
        "def load_file(name):\n",
        "    with open(name, 'r') as file:\n",
        "        return np.loadtxt(file, delimiter=',')\n",
        "\n",
        "np.set_printoptions(suppress=True, threshold=10000)\n",
        "\n",
        "train = load_file('train.csv')\n",
        "test_x = load_file('test_x.csv')\n",
        "test_y = load_file('test_y.csv')\n",
        "\n",
        "# Combine the X values to normalize them, then split them back out\n",
        "all_x = np.concatenate((train[:, 2:26], test_x[:, 2:26]))\n",
        "all_x = normalize(all_x, axis=0)\n",
        "\n",
        "train[:, 2:26] = all_x[0:train.shape[0], :]\n",
        "test_x[:, 2:26] = all_x[train.shape[0]:, :]\n",
        "\n",
        "# Make engine numbers and days zero-indexed, for everybody's sanity\n",
        "train[:, 0:2] -= 1\n",
        "test_x[:, 0:2] -= 1\n",
        "\n",
        "# Configurable observation look-back period for each engine/day\n",
        "max_time = 100\n",
        "\n",
        "def build_data(engine, time, x, max_time, is_test):\n",
        "    # y[0] will be days remaining, y[1] will be event indicator, always 1 for this data\n",
        "    out_y = np.empty((0, 2), dtype=np.float32)\n",
        "\n",
        "    # A full history of sensor readings to date for each x\n",
        "    out_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
        "\n",
        "    for i in range(100):\n",
        "        print(\"Engine: \" + str(i))\n",
        "        # When did the engine fail? (Last day + 1 for train data, irrelevant for test.)\n",
        "        max_engine_time = int(np.max(time[engine == i])) + 1\n",
        "\n",
        "        if is_test:\n",
        "            start = max_engine_time - 1\n",
        "        else:\n",
        "            start = 0\n",
        "\n",
        "        this_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
        "\n",
        "        for j in range(start, max_engine_time):\n",
        "            engine_x = x[engine == i]\n",
        "\n",
        "            out_y = np.append(out_y, np.array((max_engine_time - j, 1), ndmin=2), axis=0)\n",
        "\n",
        "            xtemp = np.zeros((1, max_time, 24))\n",
        "            xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
        "            this_x = np.concatenate((this_x, xtemp))\n",
        "\n",
        "        out_x = np.concatenate((out_x, this_x))\n",
        "\n",
        "    return out_x, out_y\n",
        "\n",
        "train_x, train_y = build_data(train[:, 0], train[:, 1], train[:, 2:26], max_time, False)\n",
        "test_x = build_data(test_x[:, 0], test_x[:, 1], test_x[:, 2:26], max_time, True)[0]\n",
        "\n",
        "train_u = np.zeros((100, 1), dtype=np.float32)\n",
        "train_u += 1\n",
        "test_y = np.append(np.reshape(test_y, (100, 1)), train_u, axis=1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Engine: 0\n",
            "Engine: 1\n",
            "Engine: 2\n",
            "Engine: 3\n",
            "Engine: 4\n",
            "Engine: 5\n",
            "Engine: 6\n",
            "Engine: 7\n",
            "Engine: 8\n",
            "Engine: 9\n",
            "Engine: 10\n",
            "Engine: 11\n",
            "Engine: 12\n",
            "Engine: 13\n",
            "Engine: 14\n",
            "Engine: 15\n",
            "Engine: 16\n",
            "Engine: 17\n",
            "Engine: 18\n",
            "Engine: 19\n",
            "Engine: 20\n",
            "Engine: 21\n",
            "Engine: 22\n",
            "Engine: 23\n",
            "Engine: 24\n",
            "Engine: 25\n",
            "Engine: 26\n",
            "Engine: 27\n",
            "Engine: 28\n",
            "Engine: 29\n",
            "Engine: 30\n",
            "Engine: 31\n",
            "Engine: 32\n",
            "Engine: 33\n",
            "Engine: 34\n",
            "Engine: 35\n",
            "Engine: 36\n",
            "Engine: 37\n",
            "Engine: 38\n",
            "Engine: 39\n",
            "Engine: 40\n",
            "Engine: 41\n",
            "Engine: 42\n",
            "Engine: 43\n",
            "Engine: 44\n",
            "Engine: 45\n",
            "Engine: 46\n",
            "Engine: 47\n",
            "Engine: 48\n",
            "Engine: 49\n",
            "Engine: 50\n",
            "Engine: 51\n",
            "Engine: 52\n",
            "Engine: 53\n",
            "Engine: 54\n",
            "Engine: 55\n",
            "Engine: 56\n",
            "Engine: 57\n",
            "Engine: 58\n",
            "Engine: 59\n",
            "Engine: 60\n",
            "Engine: 61\n",
            "Engine: 62\n",
            "Engine: 63\n",
            "Engine: 64\n",
            "Engine: 65\n",
            "Engine: 66\n",
            "Engine: 67\n",
            "Engine: 68\n",
            "Engine: 69\n",
            "Engine: 70\n",
            "Engine: 71\n",
            "Engine: 72\n",
            "Engine: 73\n",
            "Engine: 74\n",
            "Engine: 75\n",
            "Engine: 76\n",
            "Engine: 77\n",
            "Engine: 78\n",
            "Engine: 79\n",
            "Engine: 80\n",
            "Engine: 81\n",
            "Engine: 82\n",
            "Engine: 83\n",
            "Engine: 84\n",
            "Engine: 85\n",
            "Engine: 86\n",
            "Engine: 87\n",
            "Engine: 88\n",
            "Engine: 89\n",
            "Engine: 90\n",
            "Engine: 91\n",
            "Engine: 92\n",
            "Engine: 93\n",
            "Engine: 94\n",
            "Engine: 95\n",
            "Engine: 96\n",
            "Engine: 97\n",
            "Engine: 98\n",
            "Engine: 99\n",
            "Engine: 0\n",
            "Engine: 1\n",
            "Engine: 2\n",
            "Engine: 3\n",
            "Engine: 4\n",
            "Engine: 5\n",
            "Engine: 6\n",
            "Engine: 7\n",
            "Engine: 8\n",
            "Engine: 9\n",
            "Engine: 10\n",
            "Engine: 11\n",
            "Engine: 12\n",
            "Engine: 13\n",
            "Engine: 14\n",
            "Engine: 15\n",
            "Engine: 16\n",
            "Engine: 17\n",
            "Engine: 18\n",
            "Engine: 19\n",
            "Engine: 20\n",
            "Engine: 21\n",
            "Engine: 22\n",
            "Engine: 23\n",
            "Engine: 24\n",
            "Engine: 25\n",
            "Engine: 26\n",
            "Engine: 27\n",
            "Engine: 28\n",
            "Engine: 29\n",
            "Engine: 30\n",
            "Engine: 31\n",
            "Engine: 32\n",
            "Engine: 33\n",
            "Engine: 34\n",
            "Engine: 35\n",
            "Engine: 36\n",
            "Engine: 37\n",
            "Engine: 38\n",
            "Engine: 39\n",
            "Engine: 40\n",
            "Engine: 41\n",
            "Engine: 42\n",
            "Engine: 43\n",
            "Engine: 44\n",
            "Engine: 45\n",
            "Engine: 46\n",
            "Engine: 47\n",
            "Engine: 48\n",
            "Engine: 49\n",
            "Engine: 50\n",
            "Engine: 51\n",
            "Engine: 52\n",
            "Engine: 53\n",
            "Engine: 54\n",
            "Engine: 55\n",
            "Engine: 56\n",
            "Engine: 57\n",
            "Engine: 58\n",
            "Engine: 59\n",
            "Engine: 60\n",
            "Engine: 61\n",
            "Engine: 62\n",
            "Engine: 63\n",
            "Engine: 64\n",
            "Engine: 65\n",
            "Engine: 66\n",
            "Engine: 67\n",
            "Engine: 68\n",
            "Engine: 69\n",
            "Engine: 70\n",
            "Engine: 71\n",
            "Engine: 72\n",
            "Engine: 73\n",
            "Engine: 74\n",
            "Engine: 75\n",
            "Engine: 76\n",
            "Engine: 77\n",
            "Engine: 78\n",
            "Engine: 79\n",
            "Engine: 80\n",
            "Engine: 81\n",
            "Engine: 82\n",
            "Engine: 83\n",
            "Engine: 84\n",
            "Engine: 85\n",
            "Engine: 86\n",
            "Engine: 87\n",
            "Engine: 88\n",
            "Engine: 89\n",
            "Engine: 90\n",
            "Engine: 91\n",
            "Engine: 92\n",
            "Engine: 93\n",
            "Engine: 94\n",
            "Engine: 95\n",
            "Engine: 96\n",
            "Engine: 97\n",
            "Engine: 98\n",
            "Engine: 99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rnnaDAc2dsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd5f3a3b-6352-400f-e438-207136dde407"
      },
      "source": [
        "\"\"\"\n",
        "모델 피팅. \n",
        "1) 모델 시퀀스 틀 만들기 \n",
        "2) 룩백할 과거 데이터에 0을 채워 주기 (?) \n",
        "3) 시간의 순서에 따른 요소들을 신경망에 엮어주기 위해 LSTM을 설정해주기 \n",
        "4) weibull분포를 위해서 알파와 베타값을 설정해 주기위해 아웃풋에 뉴런 2개를 설정해 주기 \n",
        "5) 위에서 만든 활성함수를 불러준다 \n",
        "6) 위에서 만든 웨이블분포를 사용한 생존분석의 이산 로그우도비를 손실함수로 설정해 주기 \n",
        "7) Fit \n",
        "    Here's the rest of the meat of the demo... actually fitting and training the model.\n",
        "    We'll also make some test predictions so we can evaluate model performance.\n",
        "\"\"\"\n",
        "model = Sequential() \n",
        "model.add(Masking(mask_value=0., input_shape=(max_time, 24)))\n",
        "model.add(LSTM(20, input_dim=24))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation(activate))\n",
        "model.compile(loss=weibull_loglik_discrete, optimizer=RMSprop(lr=.001))\n",
        "model.fit(train_x, train_y, epochs=250, batch_size=2000, verbose=2, validation_data=(test_x, test_y))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/250\n",
            "11/11 - 6s - loss: 19.9016 - val_loss: 7.9731\n",
            "Epoch 2/250\n",
            "11/11 - 4s - loss: 9.2199 - val_loss: 7.2690\n",
            "Epoch 3/250\n",
            "11/11 - 5s - loss: 8.7403 - val_loss: 7.2124\n",
            "Epoch 4/250\n",
            "11/11 - 5s - loss: 8.4702 - val_loss: 7.1638\n",
            "Epoch 5/250\n",
            "11/11 - 5s - loss: 8.2366 - val_loss: 7.1017\n",
            "Epoch 6/250\n",
            "11/11 - 4s - loss: 8.0246 - val_loss: 7.0276\n",
            "Epoch 7/250\n",
            "11/11 - 5s - loss: 7.8183 - val_loss: 6.9251\n",
            "Epoch 8/250\n",
            "11/11 - 5s - loss: 7.6070 - val_loss: 6.7731\n",
            "Epoch 9/250\n",
            "11/11 - 4s - loss: 7.3724 - val_loss: 6.5487\n",
            "Epoch 10/250\n",
            "11/11 - 4s - loss: 7.1216 - val_loss: 6.3170\n",
            "Epoch 11/250\n",
            "11/11 - 5s - loss: 6.8380 - val_loss: 5.9697\n",
            "Epoch 12/250\n",
            "11/11 - 4s - loss: 6.4631 - val_loss: 5.5627\n",
            "Epoch 13/250\n",
            "11/11 - 5s - loss: 6.1068 - val_loss: 5.3455\n",
            "Epoch 14/250\n",
            "11/11 - 4s - loss: 5.8991 - val_loss: 5.2762\n",
            "Epoch 15/250\n",
            "11/11 - 5s - loss: 5.8113 - val_loss: 5.2494\n",
            "Epoch 16/250\n",
            "11/11 - 5s - loss: 5.7505 - val_loss: 5.1988\n",
            "Epoch 17/250\n",
            "11/11 - 5s - loss: 5.8126 - val_loss: 5.3431\n",
            "Epoch 18/250\n",
            "11/11 - 5s - loss: 5.7530 - val_loss: 5.2583\n",
            "Epoch 19/250\n",
            "11/11 - 4s - loss: 5.7280 - val_loss: 5.2290\n",
            "Epoch 20/250\n",
            "11/11 - 4s - loss: 5.6920 - val_loss: 5.1384\n",
            "Epoch 21/250\n",
            "11/11 - 4s - loss: 5.7582 - val_loss: 5.2961\n",
            "Epoch 22/250\n",
            "11/11 - 5s - loss: 5.6931 - val_loss: 5.1910\n",
            "Epoch 23/250\n",
            "11/11 - 4s - loss: 5.6569 - val_loss: 5.2994\n",
            "Epoch 24/250\n",
            "11/11 - 4s - loss: 5.7108 - val_loss: 5.3726\n",
            "Epoch 25/250\n",
            "11/11 - 5s - loss: 5.6717 - val_loss: 5.1883\n",
            "Epoch 26/250\n",
            "11/11 - 4s - loss: 5.6390 - val_loss: 5.1664\n",
            "Epoch 27/250\n",
            "11/11 - 4s - loss: 5.6446 - val_loss: 5.1960\n",
            "Epoch 28/250\n",
            "11/11 - 4s - loss: 5.7040 - val_loss: 5.2104\n",
            "Epoch 29/250\n",
            "11/11 - 4s - loss: 5.6074 - val_loss: 5.1055\n",
            "Epoch 30/250\n",
            "11/11 - 5s - loss: 5.6142 - val_loss: 5.4445\n",
            "Epoch 31/250\n",
            "11/11 - 5s - loss: 5.6194 - val_loss: 5.1102\n",
            "Epoch 32/250\n",
            "11/11 - 4s - loss: 5.6021 - val_loss: 5.1771\n",
            "Epoch 33/250\n",
            "11/11 - 4s - loss: 5.5972 - val_loss: 5.1268\n",
            "Epoch 34/250\n",
            "11/11 - 4s - loss: 5.5788 - val_loss: 5.1570\n",
            "Epoch 35/250\n",
            "11/11 - 4s - loss: 5.5740 - val_loss: 5.1684\n",
            "Epoch 36/250\n",
            "11/11 - 4s - loss: 5.5447 - val_loss: 5.1692\n",
            "Epoch 37/250\n",
            "11/11 - 5s - loss: 5.5446 - val_loss: 5.1114\n",
            "Epoch 38/250\n",
            "11/11 - 4s - loss: 5.5665 - val_loss: 5.1527\n",
            "Epoch 39/250\n",
            "11/11 - 5s - loss: 5.5253 - val_loss: 5.1243\n",
            "Epoch 40/250\n",
            "11/11 - 4s - loss: 5.5232 - val_loss: 5.1259\n",
            "Epoch 41/250\n",
            "11/11 - 4s - loss: 5.5226 - val_loss: 5.1315\n",
            "Epoch 42/250\n",
            "11/11 - 4s - loss: 5.4772 - val_loss: 5.1626\n",
            "Epoch 43/250\n",
            "11/11 - 4s - loss: 5.4966 - val_loss: 5.1123\n",
            "Epoch 44/250\n",
            "11/11 - 4s - loss: 5.4954 - val_loss: 5.1208\n",
            "Epoch 45/250\n",
            "11/11 - 4s - loss: 5.4849 - val_loss: 5.1227\n",
            "Epoch 46/250\n",
            "11/11 - 4s - loss: 5.5068 - val_loss: 5.1395\n",
            "Epoch 47/250\n",
            "11/11 - 4s - loss: 5.4974 - val_loss: 5.1323\n",
            "Epoch 48/250\n",
            "11/11 - 4s - loss: 5.4469 - val_loss: 5.1199\n",
            "Epoch 49/250\n",
            "11/11 - 4s - loss: 5.5181 - val_loss: 5.1460\n",
            "Epoch 50/250\n",
            "11/11 - 4s - loss: 5.4568 - val_loss: 5.1116\n",
            "Epoch 51/250\n",
            "11/11 - 5s - loss: 5.4953 - val_loss: 5.1312\n",
            "Epoch 52/250\n",
            "11/11 - 5s - loss: 5.4191 - val_loss: 5.0930\n",
            "Epoch 53/250\n",
            "11/11 - 5s - loss: 5.4529 - val_loss: 5.1115\n",
            "Epoch 54/250\n",
            "11/11 - 5s - loss: 5.4157 - val_loss: 5.1270\n",
            "Epoch 55/250\n",
            "11/11 - 4s - loss: 5.4542 - val_loss: 5.1199\n",
            "Epoch 56/250\n",
            "11/11 - 5s - loss: 5.4279 - val_loss: 5.1424\n",
            "Epoch 57/250\n",
            "11/11 - 4s - loss: 5.4351 - val_loss: 5.1286\n",
            "Epoch 58/250\n",
            "11/11 - 4s - loss: 5.4042 - val_loss: 5.1589\n",
            "Epoch 59/250\n",
            "11/11 - 4s - loss: 5.4138 - val_loss: 5.1233\n",
            "Epoch 60/250\n",
            "11/11 - 5s - loss: 5.3705 - val_loss: 5.4026\n",
            "Epoch 61/250\n",
            "11/11 - 4s - loss: 5.4824 - val_loss: 5.1155\n",
            "Epoch 62/250\n",
            "11/11 - 5s - loss: 5.3727 - val_loss: 5.1708\n",
            "Epoch 63/250\n",
            "11/11 - 4s - loss: 5.4292 - val_loss: 5.1162\n",
            "Epoch 64/250\n",
            "11/11 - 4s - loss: 5.3505 - val_loss: 5.0890\n",
            "Epoch 65/250\n",
            "11/11 - 4s - loss: 5.3783 - val_loss: 5.1564\n",
            "Epoch 66/250\n",
            "11/11 - 4s - loss: 5.3909 - val_loss: 5.1092\n",
            "Epoch 67/250\n",
            "11/11 - 4s - loss: 5.3519 - val_loss: 5.1351\n",
            "Epoch 68/250\n",
            "11/11 - 4s - loss: 5.3751 - val_loss: 5.1093\n",
            "Epoch 69/250\n",
            "11/11 - 4s - loss: 5.4062 - val_loss: 5.1508\n",
            "Epoch 70/250\n",
            "11/11 - 4s - loss: 5.3410 - val_loss: 5.1103\n",
            "Epoch 71/250\n",
            "11/11 - 4s - loss: 5.3700 - val_loss: 5.1454\n",
            "Epoch 72/250\n",
            "11/11 - 4s - loss: 5.3652 - val_loss: 5.1368\n",
            "Epoch 73/250\n",
            "11/11 - 4s - loss: 5.3661 - val_loss: 5.1894\n",
            "Epoch 74/250\n",
            "11/11 - 4s - loss: 5.3564 - val_loss: 5.4316\n",
            "Epoch 75/250\n",
            "11/11 - 4s - loss: 5.4150 - val_loss: 5.1175\n",
            "Epoch 76/250\n",
            "11/11 - 4s - loss: 5.3106 - val_loss: 5.1568\n",
            "Epoch 77/250\n",
            "11/11 - 4s - loss: 5.3588 - val_loss: 5.1110\n",
            "Epoch 78/250\n",
            "11/11 - 4s - loss: 5.3760 - val_loss: 5.1346\n",
            "Epoch 79/250\n",
            "11/11 - 5s - loss: 5.2987 - val_loss: 5.0849\n",
            "Epoch 80/250\n",
            "11/11 - 5s - loss: 5.3331 - val_loss: 5.0970\n",
            "Epoch 81/250\n",
            "11/11 - 4s - loss: 5.3218 - val_loss: 5.1499\n",
            "Epoch 82/250\n",
            "11/11 - 4s - loss: 5.3402 - val_loss: 5.1114\n",
            "Epoch 83/250\n",
            "11/11 - 4s - loss: 5.3408 - val_loss: 5.1126\n",
            "Epoch 84/250\n",
            "11/11 - 4s - loss: 5.3345 - val_loss: 5.1272\n",
            "Epoch 85/250\n",
            "11/11 - 4s - loss: 5.3176 - val_loss: 5.2115\n",
            "Epoch 86/250\n",
            "11/11 - 4s - loss: 5.3468 - val_loss: 5.0876\n",
            "Epoch 87/250\n",
            "11/11 - 4s - loss: 5.3225 - val_loss: 5.1097\n",
            "Epoch 88/250\n",
            "11/11 - 4s - loss: 5.3215 - val_loss: 5.0787\n",
            "Epoch 89/250\n",
            "11/11 - 4s - loss: 5.3225 - val_loss: 5.0855\n",
            "Epoch 90/250\n",
            "11/11 - 4s - loss: 5.3382 - val_loss: 5.0888\n",
            "Epoch 91/250\n",
            "11/11 - 4s - loss: 5.3091 - val_loss: 5.1066\n",
            "Epoch 92/250\n",
            "11/11 - 5s - loss: 5.3124 - val_loss: 5.1036\n",
            "Epoch 93/250\n",
            "11/11 - 5s - loss: 5.3198 - val_loss: 5.0849\n",
            "Epoch 94/250\n",
            "11/11 - 5s - loss: 5.3237 - val_loss: 5.1066\n",
            "Epoch 95/250\n",
            "11/11 - 4s - loss: 5.2968 - val_loss: 5.0875\n",
            "Epoch 96/250\n",
            "11/11 - 4s - loss: 5.3015 - val_loss: 5.1230\n",
            "Epoch 97/250\n",
            "11/11 - 4s - loss: 5.2927 - val_loss: 5.1496\n",
            "Epoch 98/250\n",
            "11/11 - 4s - loss: 5.3409 - val_loss: 5.0936\n",
            "Epoch 99/250\n",
            "11/11 - 4s - loss: 5.2862 - val_loss: 5.1137\n",
            "Epoch 100/250\n",
            "11/11 - 4s - loss: 5.2928 - val_loss: 5.0996\n",
            "Epoch 101/250\n",
            "11/11 - 4s - loss: 5.2902 - val_loss: 5.5005\n",
            "Epoch 102/250\n",
            "11/11 - 4s - loss: 5.3734 - val_loss: 5.0954\n",
            "Epoch 103/250\n",
            "11/11 - 4s - loss: 5.2727 - val_loss: 5.0841\n",
            "Epoch 104/250\n",
            "11/11 - 4s - loss: 5.2814 - val_loss: 5.0818\n",
            "Epoch 105/250\n",
            "11/11 - 4s - loss: 5.3365 - val_loss: 5.0894\n",
            "Epoch 106/250\n",
            "11/11 - 4s - loss: 5.2796 - val_loss: 5.0848\n",
            "Epoch 107/250\n",
            "11/11 - 4s - loss: 5.3114 - val_loss: 5.0973\n",
            "Epoch 108/250\n",
            "11/11 - 4s - loss: 5.2977 - val_loss: 5.0930\n",
            "Epoch 109/250\n",
            "11/11 - 4s - loss: 5.2777 - val_loss: 5.0910\n",
            "Epoch 110/250\n",
            "11/11 - 4s - loss: 5.2989 - val_loss: 5.0999\n",
            "Epoch 111/250\n",
            "11/11 - 4s - loss: 5.3054 - val_loss: 5.0932\n",
            "Epoch 112/250\n",
            "11/11 - 4s - loss: 5.2739 - val_loss: 5.0808\n",
            "Epoch 113/250\n",
            "11/11 - 4s - loss: 5.2890 - val_loss: 5.1067\n",
            "Epoch 114/250\n",
            "11/11 - 4s - loss: 5.3274 - val_loss: 5.0916\n",
            "Epoch 115/250\n",
            "11/11 - 5s - loss: 5.2728 - val_loss: 5.0958\n",
            "Epoch 116/250\n",
            "11/11 - 5s - loss: 5.2864 - val_loss: 5.1116\n",
            "Epoch 117/250\n",
            "11/11 - 4s - loss: 5.2883 - val_loss: 5.0937\n",
            "Epoch 118/250\n",
            "11/11 - 4s - loss: 5.3114 - val_loss: 5.0985\n",
            "Epoch 119/250\n",
            "11/11 - 4s - loss: 5.2877 - val_loss: 5.0810\n",
            "Epoch 120/250\n",
            "11/11 - 4s - loss: 5.2701 - val_loss: 5.0771\n",
            "Epoch 121/250\n",
            "11/11 - 4s - loss: 5.3006 - val_loss: 5.1080\n",
            "Epoch 122/250\n",
            "11/11 - 4s - loss: 5.2773 - val_loss: 5.0940\n",
            "Epoch 123/250\n",
            "11/11 - 5s - loss: 5.2739 - val_loss: 5.1499\n",
            "Epoch 124/250\n",
            "11/11 - 4s - loss: 5.2933 - val_loss: 5.0619\n",
            "Epoch 125/250\n",
            "11/11 - 5s - loss: 5.3157 - val_loss: 5.0917\n",
            "Epoch 126/250\n",
            "11/11 - 4s - loss: 5.2774 - val_loss: 5.0751\n",
            "Epoch 127/250\n",
            "11/11 - 4s - loss: 5.2727 - val_loss: 5.0824\n",
            "Epoch 128/250\n",
            "11/11 - 4s - loss: 5.2695 - val_loss: 5.0812\n",
            "Epoch 129/250\n",
            "11/11 - 4s - loss: 5.3086 - val_loss: 5.1128\n",
            "Epoch 130/250\n",
            "11/11 - 4s - loss: 5.2685 - val_loss: 5.1175\n",
            "Epoch 131/250\n",
            "11/11 - 5s - loss: 5.2935 - val_loss: 5.0754\n",
            "Epoch 132/250\n",
            "11/11 - 4s - loss: 5.2608 - val_loss: 5.0842\n",
            "Epoch 133/250\n",
            "11/11 - 4s - loss: 5.2870 - val_loss: 5.1003\n",
            "Epoch 134/250\n",
            "11/11 - 4s - loss: 5.2802 - val_loss: 5.1107\n",
            "Epoch 135/250\n",
            "11/11 - 4s - loss: 5.3018 - val_loss: 5.0820\n",
            "Epoch 136/250\n",
            "11/11 - 4s - loss: 5.2658 - val_loss: 5.0717\n",
            "Epoch 137/250\n",
            "11/11 - 4s - loss: 5.2656 - val_loss: 5.0741\n",
            "Epoch 138/250\n",
            "11/11 - 4s - loss: 5.2665 - val_loss: 5.0560\n",
            "Epoch 139/250\n",
            "11/11 - 5s - loss: 5.2932 - val_loss: 5.0913\n",
            "Epoch 140/250\n",
            "11/11 - 4s - loss: 5.2554 - val_loss: 5.0546\n",
            "Epoch 141/250\n",
            "11/11 - 4s - loss: 5.2717 - val_loss: 5.0994\n",
            "Epoch 142/250\n",
            "11/11 - 5s - loss: 5.2776 - val_loss: 5.0900\n",
            "Epoch 143/250\n",
            "11/11 - 5s - loss: 5.2890 - val_loss: 5.0906\n",
            "Epoch 144/250\n",
            "11/11 - 5s - loss: 5.2633 - val_loss: 5.0705\n",
            "Epoch 145/250\n",
            "11/11 - 4s - loss: 5.2599 - val_loss: 5.0911\n",
            "Epoch 146/250\n",
            "11/11 - 4s - loss: 5.2748 - val_loss: 5.0814\n",
            "Epoch 147/250\n",
            "11/11 - 4s - loss: 5.2618 - val_loss: 5.0810\n",
            "Epoch 148/250\n",
            "11/11 - 5s - loss: 5.2840 - val_loss: 5.0990\n",
            "Epoch 149/250\n",
            "11/11 - 5s - loss: 5.2459 - val_loss: 5.1637\n",
            "Epoch 150/250\n",
            "11/11 - 5s - loss: 5.3092 - val_loss: 5.0865\n",
            "Epoch 151/250\n",
            "11/11 - 5s - loss: 5.2478 - val_loss: 5.0649\n",
            "Epoch 152/250\n",
            "11/11 - 5s - loss: 5.2654 - val_loss: 5.0550\n",
            "Epoch 153/250\n",
            "11/11 - 4s - loss: 5.2935 - val_loss: 5.0943\n",
            "Epoch 154/250\n",
            "11/11 - 5s - loss: 5.2441 - val_loss: 5.0732\n",
            "Epoch 155/250\n",
            "11/11 - 5s - loss: 5.2788 - val_loss: 5.1022\n",
            "Epoch 156/250\n",
            "11/11 - 5s - loss: 5.2496 - val_loss: 5.0604\n",
            "Epoch 157/250\n",
            "11/11 - 4s - loss: 5.3069 - val_loss: 5.0999\n",
            "Epoch 158/250\n",
            "11/11 - 5s - loss: 5.2509 - val_loss: 5.0854\n",
            "Epoch 159/250\n",
            "11/11 - 4s - loss: 5.2716 - val_loss: 5.0840\n",
            "Epoch 160/250\n",
            "11/11 - 5s - loss: 5.2615 - val_loss: 5.0922\n",
            "Epoch 161/250\n",
            "11/11 - 5s - loss: 5.2602 - val_loss: 5.0719\n",
            "Epoch 162/250\n",
            "11/11 - 4s - loss: 5.2646 - val_loss: 5.0914\n",
            "Epoch 163/250\n",
            "11/11 - 5s - loss: 5.2585 - val_loss: 5.0728\n",
            "Epoch 164/250\n",
            "11/11 - 5s - loss: 5.2602 - val_loss: 5.0831\n",
            "Epoch 165/250\n",
            "11/11 - 5s - loss: 5.2973 - val_loss: 5.0900\n",
            "Epoch 166/250\n",
            "11/11 - 4s - loss: 5.2555 - val_loss: 5.0516\n",
            "Epoch 167/250\n",
            "11/11 - 4s - loss: 5.2737 - val_loss: 5.0799\n",
            "Epoch 168/250\n",
            "11/11 - 4s - loss: 5.2535 - val_loss: 5.0496\n",
            "Epoch 169/250\n",
            "11/11 - 5s - loss: 5.2500 - val_loss: 5.0864\n",
            "Epoch 170/250\n",
            "11/11 - 4s - loss: 5.2910 - val_loss: 5.0930\n",
            "Epoch 171/250\n",
            "11/11 - 4s - loss: 5.2629 - val_loss: 5.0881\n",
            "Epoch 172/250\n",
            "11/11 - 4s - loss: 5.2496 - val_loss: 5.0618\n",
            "Epoch 173/250\n",
            "11/11 - 4s - loss: 5.2582 - val_loss: 5.3631\n",
            "Epoch 174/250\n",
            "11/11 - 4s - loss: 5.3184 - val_loss: 5.0864\n",
            "Epoch 175/250\n",
            "11/11 - 5s - loss: 5.2414 - val_loss: 5.0655\n",
            "Epoch 176/250\n",
            "11/11 - 4s - loss: 5.2693 - val_loss: 5.0960\n",
            "Epoch 177/250\n",
            "11/11 - 4s - loss: 5.2490 - val_loss: 5.0607\n",
            "Epoch 178/250\n",
            "11/11 - 4s - loss: 5.2565 - val_loss: 5.0706\n",
            "Epoch 179/250\n",
            "11/11 - 4s - loss: 5.2495 - val_loss: 5.2619\n",
            "Epoch 180/250\n",
            "11/11 - 4s - loss: 5.2991 - val_loss: 5.0772\n",
            "Epoch 181/250\n",
            "11/11 - 4s - loss: 5.2386 - val_loss: 5.2094\n",
            "Epoch 182/250\n",
            "11/11 - 4s - loss: 5.2956 - val_loss: 5.0847\n",
            "Epoch 183/250\n",
            "11/11 - 5s - loss: 5.2476 - val_loss: 5.0956\n",
            "Epoch 184/250\n",
            "11/11 - 5s - loss: 5.2500 - val_loss: 5.0707\n",
            "Epoch 185/250\n",
            "11/11 - 4s - loss: 5.2455 - val_loss: 5.0730\n",
            "Epoch 186/250\n",
            "11/11 - 5s - loss: 5.2586 - val_loss: 5.0440\n",
            "Epoch 187/250\n",
            "11/11 - 4s - loss: 5.2617 - val_loss: 5.0662\n",
            "Epoch 188/250\n",
            "11/11 - 4s - loss: 5.2730 - val_loss: 5.0901\n",
            "Epoch 189/250\n",
            "11/11 - 5s - loss: 5.2451 - val_loss: 5.0548\n",
            "Epoch 190/250\n",
            "11/11 - 4s - loss: 5.2594 - val_loss: 5.0754\n",
            "Epoch 191/250\n",
            "11/11 - 4s - loss: 5.2475 - val_loss: 5.0550\n",
            "Epoch 192/250\n",
            "11/11 - 4s - loss: 5.2733 - val_loss: 5.1114\n",
            "Epoch 193/250\n",
            "11/11 - 4s - loss: 5.2517 - val_loss: 5.0665\n",
            "Epoch 194/250\n",
            "11/11 - 4s - loss: 5.2763 - val_loss: 5.0647\n",
            "Epoch 195/250\n",
            "11/11 - 4s - loss: 5.2465 - val_loss: 5.0626\n",
            "Epoch 196/250\n",
            "11/11 - 4s - loss: 5.2742 - val_loss: 5.0748\n",
            "Epoch 197/250\n",
            "11/11 - 5s - loss: 5.2370 - val_loss: 5.0502\n",
            "Epoch 198/250\n",
            "11/11 - 4s - loss: 5.2438 - val_loss: 5.2171\n",
            "Epoch 199/250\n",
            "11/11 - 5s - loss: 5.2971 - val_loss: 5.0978\n",
            "Epoch 200/250\n",
            "11/11 - 5s - loss: 5.2357 - val_loss: 5.0623\n",
            "Epoch 201/250\n",
            "11/11 - 5s - loss: 5.2435 - val_loss: 5.0707\n",
            "Epoch 202/250\n",
            "11/11 - 4s - loss: 5.2434 - val_loss: 5.0467\n",
            "Epoch 203/250\n",
            "11/11 - 4s - loss: 5.2507 - val_loss: 5.0437\n",
            "Epoch 204/250\n",
            "11/11 - 5s - loss: 5.2762 - val_loss: 5.0735\n",
            "Epoch 205/250\n",
            "11/11 - 5s - loss: 5.2520 - val_loss: 5.0739\n",
            "Epoch 206/250\n",
            "11/11 - 5s - loss: 5.2631 - val_loss: 5.0636\n",
            "Epoch 207/250\n",
            "11/11 - 4s - loss: 5.2649 - val_loss: 5.0778\n",
            "Epoch 208/250\n",
            "11/11 - 4s - loss: 5.2371 - val_loss: 5.0524\n",
            "Epoch 209/250\n",
            "11/11 - 4s - loss: 5.2500 - val_loss: 5.0736\n",
            "Epoch 210/250\n",
            "11/11 - 4s - loss: 5.2505 - val_loss: 5.0903\n",
            "Epoch 211/250\n",
            "11/11 - 4s - loss: 5.2409 - val_loss: 5.0627\n",
            "Epoch 212/250\n",
            "11/11 - 4s - loss: 5.2564 - val_loss: 5.0774\n",
            "Epoch 213/250\n",
            "11/11 - 4s - loss: 5.2478 - val_loss: 5.0614\n",
            "Epoch 214/250\n",
            "11/11 - 4s - loss: 5.2480 - val_loss: 5.0843\n",
            "Epoch 215/250\n",
            "11/11 - 5s - loss: 5.2790 - val_loss: 5.0886\n",
            "Epoch 216/250\n",
            "11/11 - 5s - loss: 5.2350 - val_loss: 5.0464\n",
            "Epoch 217/250\n",
            "11/11 - 4s - loss: 5.2459 - val_loss: 5.1016\n",
            "Epoch 218/250\n",
            "11/11 - 4s - loss: 5.2488 - val_loss: 5.0427\n",
            "Epoch 219/250\n",
            "11/11 - 4s - loss: 5.2446 - val_loss: 5.2915\n",
            "Epoch 220/250\n",
            "11/11 - 4s - loss: 5.2973 - val_loss: 5.0846\n",
            "Epoch 221/250\n",
            "11/11 - 4s - loss: 5.2357 - val_loss: 5.0819\n",
            "Epoch 222/250\n",
            "11/11 - 4s - loss: 5.2327 - val_loss: 5.2927\n",
            "Epoch 223/250\n",
            "11/11 - 4s - loss: 5.2953 - val_loss: 5.0825\n",
            "Epoch 224/250\n",
            "11/11 - 4s - loss: 5.2341 - val_loss: 5.0836\n",
            "Epoch 225/250\n",
            "11/11 - 4s - loss: 5.2396 - val_loss: 5.0868\n",
            "Epoch 226/250\n",
            "11/11 - 5s - loss: 5.2554 - val_loss: 5.0811\n",
            "Epoch 227/250\n",
            "11/11 - 4s - loss: 5.2633 - val_loss: 5.0930\n",
            "Epoch 228/250\n",
            "11/11 - 5s - loss: 5.2393 - val_loss: 5.0552\n",
            "Epoch 229/250\n",
            "11/11 - 4s - loss: 5.2625 - val_loss: 5.0876\n",
            "Epoch 230/250\n",
            "11/11 - 4s - loss: 5.2303 - val_loss: 5.0756\n",
            "Epoch 231/250\n",
            "11/11 - 4s - loss: 5.2510 - val_loss: 5.0847\n",
            "Epoch 232/250\n",
            "11/11 - 4s - loss: 5.2380 - val_loss: 5.0731\n",
            "Epoch 233/250\n",
            "11/11 - 4s - loss: 5.2534 - val_loss: 5.0921\n",
            "Epoch 234/250\n",
            "11/11 - 4s - loss: 5.2428 - val_loss: 5.0845\n",
            "Epoch 235/250\n",
            "11/11 - 4s - loss: 5.2343 - val_loss: 5.0463\n",
            "Epoch 236/250\n",
            "11/11 - 4s - loss: 5.2440 - val_loss: 5.0428\n",
            "Epoch 237/250\n",
            "11/11 - 5s - loss: 5.2605 - val_loss: 5.1822\n",
            "Epoch 238/250\n",
            "11/11 - 5s - loss: 5.2720 - val_loss: 5.0880\n",
            "Epoch 239/250\n",
            "11/11 - 5s - loss: 5.2296 - val_loss: 5.0366\n",
            "Epoch 240/250\n",
            "11/11 - 4s - loss: 5.2468 - val_loss: 5.0604\n",
            "Epoch 241/250\n",
            "11/11 - 5s - loss: 5.2722 - val_loss: 5.0811\n",
            "Epoch 242/250\n",
            "11/11 - 4s - loss: 5.2434 - val_loss: 5.0562\n",
            "Epoch 243/250\n",
            "11/11 - 4s - loss: 5.2336 - val_loss: 5.0660\n",
            "Epoch 244/250\n",
            "11/11 - 4s - loss: 5.2694 - val_loss: 5.0763\n",
            "Epoch 245/250\n",
            "11/11 - 4s - loss: 5.2374 - val_loss: 5.0706\n",
            "Epoch 246/250\n",
            "11/11 - 4s - loss: 5.2529 - val_loss: 5.0821\n",
            "Epoch 247/250\n",
            "11/11 - 5s - loss: 5.2344 - val_loss: 5.1926\n",
            "Epoch 248/250\n",
            "11/11 - 4s - loss: 5.2782 - val_loss: 5.0904\n",
            "Epoch 249/250\n",
            "11/11 - 4s - loss: 5.2296 - val_loss: 5.0817\n",
            "Epoch 250/250\n",
            "11/11 - 4s - loss: 5.2435 - val_loss: 5.1612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe80c994f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o5T1z-J4n9W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ec4fb96f-4ad5-46b7-f689-57322e78876c"
      },
      "source": [
        "\"\"\"\n",
        "피팅한 모델로 예측하기. \n",
        "\"\"\"\n",
        "test_predict = model.predict(test_x)\n",
        "test_predict = np.resize(test_predict, (100, 2))\n",
        "test_result = np.concatenate((test_y, test_predict), axis=1)\n",
        "print(test_result)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[112.           1.         198.66978455   4.11230183]\n",
            " [ 98.           1.         177.6108551    3.71354365]\n",
            " [ 69.           1.          92.75230408   1.73186457]\n",
            " [ 82.           1.         100.31937408   1.86831474]\n",
            " [ 91.           1.         117.66693115   2.18653274]\n",
            " [ 93.           1.          87.44530487   1.67709649]\n",
            " [ 91.           1.          95.11090851   1.80621028]\n",
            " [ 95.           1.         100.22809601   1.90449286]\n",
            " [111.           1.         176.24464417   3.57753158]\n",
            " [ 96.           1.          92.14980316   1.71814716]\n",
            " [ 97.           1.         151.41093445   2.99863911]\n",
            " [124.           1.          92.54492188   1.7398957 ]\n",
            " [ 95.           1.          95.70077515   1.82191455]\n",
            " [107.           1.         179.0954895    3.76012683]\n",
            " [ 83.           1.         154.0289917    3.15142584]\n",
            " [ 84.           1.          99.24423218   1.85282183]\n",
            " [ 50.           1.          96.34508514   1.81977439]\n",
            " [ 28.           1.         102.03434753   1.90822136]\n",
            " [ 87.           1.          95.80199432   1.79568422]\n",
            " [ 16.           1.          97.21242523   1.84499371]\n",
            " [ 57.           1.          98.89965057   1.85295641]\n",
            " [111.           1.         184.81539917   3.89915967]\n",
            " [113.           1.          93.33233643   1.74133301]\n",
            " [ 20.           1.          94.72705078   1.78132474]\n",
            " [145.           1.         177.41262817   3.7238884 ]\n",
            " [119.           1.         155.52778625   3.15141058]\n",
            " [ 66.           1.          90.70648956   1.69864011]\n",
            " [ 97.           1.          93.22216797   1.75422227]\n",
            " [ 90.           1.          97.33850098   1.8392626 ]\n",
            " [115.           1.          89.00434113   1.67554903]\n",
            " [  8.           1.          97.53149414   1.83679581]\n",
            " [ 48.           1.          89.3184433    1.685063  ]\n",
            " [106.           1.         178.87750244   3.69068909]\n",
            " [  7.           1.         105.23160553   1.97241199]\n",
            " [ 11.           1.          88.30795288   1.66921186]\n",
            " [ 19.           1.          99.84387207   1.88478935]\n",
            " [ 21.           1.          97.72543335   1.82821155]\n",
            " [ 50.           1.          95.25501251   1.81169689]\n",
            " [142.           1.         191.31915283   3.97150087]\n",
            " [ 28.           1.         102.60444641   1.94262338]\n",
            " [ 18.           1.          91.00485229   1.7145493 ]\n",
            " [ 10.           1.         106.56784058   1.98646927]\n",
            " [ 59.           1.          97.46353149   1.82229769]\n",
            " [109.           1.         175.95510864   3.61985016]\n",
            " [114.           1.          91.52297974   1.72816968]\n",
            " [ 47.           1.          99.23192596   1.88047433]\n",
            " [135.           1.         153.25169373   3.18006563]\n",
            " [ 92.           1.         152.06422424   3.12555671]\n",
            " [ 21.           1.          94.9016571    1.76799405]\n",
            " [ 79.           1.         155.67848206   3.16907263]\n",
            " [114.           1.          93.64505768   1.74656332]\n",
            " [ 29.           1.          91.76184082   1.74511981]\n",
            " [ 26.           1.          88.91716766   1.69231558]\n",
            " [ 97.           1.          94.58085632   1.78428924]\n",
            " [137.           1.          87.72164917   1.64246964]\n",
            " [ 15.           1.          96.52957916   1.83335948]\n",
            " [103.           1.          95.25959778   1.80115199]\n",
            " [ 37.           1.          98.85250092   1.83671403]\n",
            " [114.           1.         134.51377869   2.49373198]\n",
            " [100.           1.          92.88299561   1.76463938]\n",
            " [ 21.           1.         100.92324829   1.88754177]\n",
            " [ 54.           1.          94.9099884    1.80837464]\n",
            " [ 72.           1.          96.91288757   1.83203423]\n",
            " [ 28.           1.          93.0429306    1.74524128]\n",
            " [128.           1.         155.45439148   3.18829823]\n",
            " [ 14.           1.          91.09281921   1.72729707]\n",
            " [ 77.           1.         154.78269958   3.18376017]\n",
            " [  8.           1.          97.19101715   1.8382014 ]\n",
            " [121.           1.         175.45477295   3.60035062]\n",
            " [ 94.           1.         101.38040161   1.91238117]\n",
            " [118.           1.         157.98573303   3.20956445]\n",
            " [ 50.           1.          94.21136475   1.79055703]\n",
            " [131.           1.          93.97079468   1.77631927]\n",
            " [126.           1.         101.58533478   1.9246639 ]\n",
            " [113.           1.         145.04327393   2.78548265]\n",
            " [ 10.           1.          91.93772888   1.7364558 ]\n",
            " [ 34.           1.         102.59284973   1.90113986]\n",
            " [107.           1.         157.35270691   3.17692447]\n",
            " [ 63.           1.          99.1572876    1.86100435]\n",
            " [ 90.           1.          88.87206268   1.67297649]\n",
            " [  8.           1.          94.66880035   1.76692092]\n",
            " [  9.           1.         100.25362396   1.88406122]\n",
            " [137.           1.         156.67019653   3.18033409]\n",
            " [ 58.           1.          89.34144592   1.68693674]\n",
            " [118.           1.         190.43981934   4.03359079]\n",
            " [ 89.           1.          93.74084473   1.7427187 ]\n",
            " [116.           1.         176.43652344   3.56584001]\n",
            " [115.           1.         159.08822632   3.20726037]\n",
            " [136.           1.          87.92601776   1.6754694 ]\n",
            " [ 28.           1.          89.01847076   1.66810584]\n",
            " [ 38.           1.          95.93067932   1.80491316]\n",
            " [ 20.           1.          91.40234375   1.74521887]\n",
            " [ 85.           1.         101.61891174   1.91623592]\n",
            " [ 55.           1.          95.27713013   1.7921499 ]\n",
            " [128.           1.         146.82110596   2.77903199]\n",
            " [137.           1.         120.006073     2.19290757]\n",
            " [ 82.           1.          91.62874603   1.73142815]\n",
            " [ 59.           1.          93.50575256   1.76362109]\n",
            " [117.           1.         121.51217651   2.24534249]\n",
            " [ 20.           1.         105.69713593   1.95965111]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijU3mr2UEG1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}